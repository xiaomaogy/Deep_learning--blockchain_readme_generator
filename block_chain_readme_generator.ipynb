{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blockchain README.md generator\n",
    "\n",
    "There are a lot of bitcoin assets listed on https://coinmarketcap.com/, and about 80% of them have corresponding github and the corresponding introduction in their README.md. If you look through them, you will find all of them looks similar. So I figured maybe I can train an RNN network that automatically generate my own coin's README.md. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_and_save_data(dataset_path, token_lookup, create_lookup_tables):\n",
    "    \"\"\"\n",
    "    Preprocess Text Data\n",
    "    \"\"\"\n",
    "    text = load_data(dataset_path)\n",
    "    \n",
    "    token_dict = token_lookup()\n",
    "    for key, token in token_dict.items():\n",
    "        text = text.replace(key, ' {} '.format(token))\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(text)\n",
    "    int_text = [vocab_to_int[word] for word in text]\n",
    "    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_preprocess():\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    return pickle.load(open('preprocess.p', mode='rb'))\n",
    "\n",
    "\n",
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = './data/blockchain.txt'\n",
    "text = load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create word embedding to convert words into int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    words = set(text)\n",
    "    vocab_to_int = {word: idx for idx, word in enumerate(words)}\n",
    "    int_to_vocab = {idx: word for word, idx in vocab_to_int.items()}\n",
    "    return vocab_to_int, int_to_vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make punctuation into separate words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    res = {'.': \"||Period||\",\n",
    "                       ',': \"||Comma||\", \n",
    "                      '\"': \"||Quotation_mark||\", \n",
    "                      ';': \"||Semicolon||\", \n",
    "                      '!': \"||Exclamation_mark||\", \n",
    "                      '?': \"||Question_mark||\", \n",
    "                      '(': \"||Left_Parentheses||\", \n",
    "                      ')': \"||Right_Parentheses||\", \n",
    "                      '--': \"||Dash||\", \n",
    "                      '\\n': \"||Return||\", \n",
    "                      '\\t': \"|||Tab|\", \n",
    "                      '#': \"||Hash_tag||\", \n",
    "                      '$': \"||Dollar||\", \n",
    "                      '%': \"||Percent||\", \n",
    "                      '&': \"||And||\", \n",
    "                      '\\'': \"||Single_Quotation_mark||\", \n",
    "                      '*': \"||Asterisk||\", \n",
    "                      '+': \"||Plus||\", \n",
    "                      '-': \"||Minus||\", \n",
    "                      '/': \"||Slash||\", \n",
    "                      '<': \"||Less||\", \n",
    "                      '=': \"||Equal||\", \n",
    "                      '>': \"||Larger||\", \n",
    "                      '@': \"||At_Symbol||\", \n",
    "                       '[': \"||Left_bracket||\", \n",
    "                       ']': \"||Right_bracket||\", \n",
    "                       '©': \"||Copy_right_symbol||\", \n",
    "                       'Ð': \"||D_Symbol||\", \n",
    "                      }\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure the tensorflow version is right, and we are using gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a146cdbd02cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Check TensorFlow Version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    inputs_ = tf.placeholder(dtype = tf.int32, name='input', shape = [None, None])\n",
    "    targets_ = tf.placeholder(dtype = tf.int32, name='target', shape = [None, None])\n",
    "    learning_rate = tf.placeholder(name='lr', dtype = tf.float32)\n",
    "    return inputs_, targets_, learning_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RNN structure (cell and state, then connect cells together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_size)\n",
    "    dropout = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=0.8)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells=[dropout])\n",
    "    initial_state = tf.identity(cell.zero_state(batch_size=batch_size, dtype=tf.float32), \n",
    "                                name = 'initial_state')\n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=inputs, dtype = tf.float32)\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "    return outputs, final_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create word embedding structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocab_size, embed_dim], -1.0, 1.0))\n",
    "    \n",
    "    return tf.nn.embedding_lookup(embeddings, ids=input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect the input tensor, the word embedding structure, the rnn structure, then the fully connected layer together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    rnn_out, final_state = build_rnn(cell, embed)\n",
    "    logits = tf.contrib.layers.fully_connected(inputs=rnn_out, num_outputs=vocab_size, activation_fn=None)\n",
    "    return logits, final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a function that make the input as batches, the batch shape should be (num_batches, 2, batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    n_batches = len(int_text)//(batch_size * seq_length)\n",
    "    res = np.zeros((n_batches, 2, batch_size, seq_length))\n",
    "    \n",
    "    total_len = n_batches *batch_size*seq_length\n",
    "    inputs = np.array(int_text[:total_len])\n",
    "    targets = int_text[1:total_len]\n",
    "    targets.append(int_text[0])\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    #shape: n_batches, 2, batch_size, seq_length\n",
    "    \n",
    "    skip = n_batches * seq_length\n",
    "    for batch in range(n_batches):\n",
    "        for n in range(batch_size):\n",
    "            res[batch, 0, n] = inputs[range(batch * seq_length + n * skip, batch * seq_length + n * skip + seq_length)]\n",
    "            res[batch, 1, n] = targets[range(batch * seq_length + n * skip, batch * seq_length + n * skip + seq_length)]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all the hyperparameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 100\n",
    "# Batch Size\n",
    "batch_size = 100\n",
    "# RNN Size\n",
    "rnn_size = 528\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 500\n",
    "# Sequence Length\n",
    "seq_length = 50\n",
    "# Learning Rate\n",
    "learning_rate = 0.005\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 10\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the actual graph with the functions we defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/4   train_loss = 7.964\n",
      "Epoch   2 Batch    2/4   train_loss = 5.079\n",
      "Epoch   5 Batch    0/4   train_loss = 3.904\n",
      "Epoch   7 Batch    2/4   train_loss = 3.246\n",
      "Epoch  10 Batch    0/4   train_loss = 2.275\n",
      "Epoch  12 Batch    2/4   train_loss = 1.824\n",
      "Epoch  15 Batch    0/4   train_loss = 1.209\n",
      "Epoch  17 Batch    2/4   train_loss = 0.939\n",
      "Epoch  20 Batch    0/4   train_loss = 0.655\n",
      "Epoch  22 Batch    2/4   train_loss = 0.492\n",
      "Epoch  25 Batch    0/4   train_loss = 0.364\n",
      "Epoch  27 Batch    2/4   train_loss = 0.276\n",
      "Epoch  30 Batch    0/4   train_loss = 0.231\n",
      "Epoch  32 Batch    2/4   train_loss = 0.184\n",
      "Epoch  35 Batch    0/4   train_loss = 0.170\n",
      "Epoch  37 Batch    2/4   train_loss = 0.138\n",
      "Epoch  40 Batch    0/4   train_loss = 0.133\n",
      "Epoch  42 Batch    2/4   train_loss = 0.103\n",
      "Epoch  45 Batch    0/4   train_loss = 0.106\n",
      "Epoch  47 Batch    2/4   train_loss = 0.081\n",
      "Epoch  50 Batch    0/4   train_loss = 0.089\n",
      "Epoch  52 Batch    2/4   train_loss = 0.071\n",
      "Epoch  55 Batch    0/4   train_loss = 0.077\n",
      "Epoch  57 Batch    2/4   train_loss = 0.061\n",
      "Epoch  60 Batch    0/4   train_loss = 0.071\n",
      "Epoch  62 Batch    2/4   train_loss = 0.061\n",
      "Epoch  65 Batch    0/4   train_loss = 0.067\n",
      "Epoch  67 Batch    2/4   train_loss = 0.054\n",
      "Epoch  70 Batch    0/4   train_loss = 0.061\n",
      "Epoch  72 Batch    2/4   train_loss = 0.049\n",
      "Epoch  75 Batch    0/4   train_loss = 0.057\n",
      "Epoch  77 Batch    2/4   train_loss = 0.043\n",
      "Epoch  80 Batch    0/4   train_loss = 0.055\n",
      "Epoch  82 Batch    2/4   train_loss = 0.043\n",
      "Epoch  85 Batch    0/4   train_loss = 0.056\n",
      "Epoch  87 Batch    2/4   train_loss = 0.042\n",
      "Epoch  90 Batch    0/4   train_loss = 0.053\n",
      "Epoch  92 Batch    2/4   train_loss = 0.038\n",
      "Epoch  95 Batch    0/4   train_loss = 0.051\n",
      "Epoch  97 Batch    2/4   train_loss = 0.039\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = load_preprocess()\n",
    "seq_length, load_dir = load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    inputs = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    return inputs, initial_state, final_state, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    idx = np.argmax(probabilities)\n",
    "    return int_to_vocab[idx]\n",
    "\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "bitcoin?\n",
      "----------------\n",
      "\n",
      "the ncc api is available as a swagger. json file[ here](docs/ ncc- api- swagger. json).\n",
      "\n",
      "a rendered version is available[ here](http:// www. boost. org/) version in the range[ 1. 57, 1. 60]. versions earlier than\n",
      "1. 57 or newer than 1. 60 are not supported. if your system boost boost\n",
      "\n",
      "### getting the code\n",
      "to download all of the code, download eos and the second line is the data.\n",
      "\n",
      "```\n",
      "npm install- g-----------\n",
      "\n",
      "we only accept translation fixes that are submitted through[ bitcoin core' s transifex page](https:// www. transifex. com/ projects/ p/ bitcoin/).\n",
      "\n",
      "periodically the translations are pulled from transifex and merged into the git repository. see the\n",
      "[ translation process](doc/ translation_process. md) for details on how this works.\n",
      "\n",
      "** important** : we do not accept translation changes as github pull requests because the next\n",
      "pull from transifex would automatically overwrite them again.\n",
      "\n",
      "# dogecoin core[ doge,Ð]\n",
      "==============================================================================================================\n",
      "\n",
      "![ dogecoin](http:// static. tumblr. com/ ppdj5y9/ ae9mxmxtp/ 300coin. png)\n",
      "\n",
      "[![ build status](https:// travis- ci. org/ litecoin- project/ litecoin. svg? branch= master)](https:// travis- ci. org/ litecoin- project/ litecoin)\n",
      "\n",
      "## license\n",
      "\n",
      "see[ license](license).\n",
      "\n",
      "# contributing\n",
      "\n",
      "see[ license](license).\n",
      "\n",
      "# contributing\n",
      "\n",
      "see[ license](license).\n",
      "\n",
      "# contributing\n",
      "\n",
      "## how to build llvm and clang for wasm\n",
      "\n",
      "by default llvm and clang do not include the wasm build wasm targets.\n",
      "\n",
      "### member naming\n",
      "- use lowercamelcase.\n",
      "- prefix booleans with\" is\"/\" has\"/\" are\" are\" on\",\n",
      "\" nonce\" :\" 0x0000000000000042\",\n",
      "\" mixhash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" parenthash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 0x2fefd8\",\n",
      "\" nonce\" :\" 0x0000000000000042\",\n",
      "\" mixhash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" parenthash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 0x2fefd8\",\n",
      "\" nonce\" :\" 0x0000000000000042\",\n",
      "\" mixhash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" parenthash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 0x2fefd8\",\n",
      "\" nonce\" :\" 0x0000000000000042\",\n",
      "\" mixhash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" parenthash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 0x2fefd8\",\n",
      "\" nonce\" :\" 0x0000000000000042\",\n",
      "\" mixhash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" parenthash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 0x2fefd8\",\n",
      "\" nonce\" :\" 0x0000000000000042\",\n",
      "\" mixhash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" parenthash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 0x2fefd8\",\n",
      "\" nonce\" :\" 0x0000000000000042\",\n",
      "\" mixhash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" parenthash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 0x2fefd8\",\n",
      "\" nonce\" :\" 0x0000000000000042\",\n",
      "\" mixhash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" parenthash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 0x2fefd8\",\n",
      "\" nonce\" :\" 0x0000000000000042\",\n",
      "\" mixhash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" parenthash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 222222222\" }\n",
      "},\n",
      "\" 0x0000000000000000000000000000000000000002\" :\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" parenthash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" timestamp\" :\" 0x2fefd8\",\n",
      "\" nonce\" :\" 0x0000000000000042\",\n",
      "\" mixhash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\" parenthash\" :\" 0x0000000000000000000000000000000000000000000000000000000000000000\",\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_length = 1000\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'bitcoin'\n",
    "\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
